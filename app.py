import os
from typing import Tuple, List

import gradio as gr
import numpy as np
import openai
from dotenv import load_dotenv

from src.Emotional_Analysis import emotional_analysis, sentiment_analysis
from src import whisper, login, llm_run, img_to_text


def on_clear_button_click() -> Tuple[str, List]:
    return "", []


def process_audio(audio):
    if audio is None:
        # ç”¨æˆ·æ²¡æœ‰ä¸Šä¼ å›¾ç‰‡ï¼Œæ˜¾ç¤ºè­¦å‘Šä¿¡æ¯
        gr.Warning("äº²ï¼Œè¯·å½•åˆ¶æˆ–ä¸Šä¼ ä¸€æ®µéŸ³é¢‘ï¼")
        return None
    else:
        # åœ¨è¿™é‡Œå¤„ç†ç”¨æˆ·ä¸Šä¼ çš„å›¾ç‰‡
        result_dict = sentiment_analysis.audio_sentiment(audio)
        return result_dict


def process_img(image):
    if image is None:
        # ç”¨æˆ·æ²¡æœ‰ä¸Šä¼ å›¾ç‰‡ï¼Œæ˜¾ç¤ºè­¦å‘Šä¿¡æ¯
        gr.Warning("äº²ï¼Œè¯·æ‹æ‘„æˆ–ä¸Šä¼ ä¸€å¼ å›¾ç‰‡ï¼")
        return None
    else:
        # åœ¨è¿™é‡Œå¤„ç†ç”¨æˆ·ä¸Šä¼ çš„å›¾ç‰‡
        result_dict = sentiment_analysis.img_sentiment(image)
        return result_dict


with gr.Blocks() as demo_Emotion:
    gr.Markdown("# æ¬¢è¿è¿›å…¥ Emotion-GPT! ğŸŒŸğŸš€")

    role = gr.Radio(label="è§’è‰²è®¾å®š",
                    choices=["æ— ", "é»˜è®¤", "å¿ƒç†åŒ»å¸ˆ", "åŠ±å¿—æ•™ç»ƒ", "éŸ³ä¹æ¨è"],
                    value="é»˜è®¤")
    chatbot = gr.Chatbot(elem_id="chatbot",
                         height="600px",
                         label="èŠå¤©åŒº")
    clear = gr.Button("æ¸…é™¤")
    state = gr.State([])
    with gr.Row():
        txt = gr.Textbox(label="æ–‡å­—è¾“å…¥",
                         placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ŒæŒ‰å›è½¦å‘é€",
                         elem_id="textbox")
        # å¾—åˆ°éŸ³é¢‘æ–‡ä»¶åœ°å€
        audio = gr.Audio(sources="microphone",
                         type="filepath",
                         label="è¯­éŸ³è¾“å…¥")
    img = gr.Image(sources="upload",
                   type="filepath",
                   label="å›¾ç‰‡è¾“å…¥")
    audio_output = gr.Audio(autoplay=True,
                            visible=False
                            )

    img.upload(img_to_text.img2text,
               [img, state],
               [chatbot, state, audio_output])
    txt.submit(llm_run.conversation_run,
               [txt, role, state],
               [chatbot, state, audio_output])
    audio.stop_recording(whisper.process_audio,
                         [audio, role, state],
                         [chatbot, state, audio_output])
    clear.click(on_clear_button_click,
                None,
                [chatbot, state],
                queue=False, )
    # audio.stop_recording(waveform.wavReader,
    #                      audio)

with gr.Blocks() as demo_EmotionalAnalysis:
    gr.Markdown("# æ¬¢è¿è¿›å…¥ æƒ…æ„Ÿåˆ†ææ¨¡å—! ğŸŒŸğŸš€")
    with gr.Row(variant="panel"):
        with gr.Column():
            txt_emotion = gr.Textbox(label="æ–‡å­—è¾“å…¥",
                                     placeholder="è¯·è¾“å…¥éœ€è¦åˆ†æçš„è¯­å¥ï¼ŒæŒ‰å›è½¦å‘é€",
                                     )
            shape = gr.Dropdown(
                ["ğ’Š¹", "â¤", "â™¦", "â–¶", "â–²", "â˜…"],
                value="â¤",
                label="è¯äº‘å½¢çŠ¶é€‰æ‹©",
                info="è¯·é€‰æ‹©å–œæ¬¢çš„å½¢çŠ¶"
            )
            up_btn = gr.UploadButton(variant="primary",
                                     label="ä¸Šä¼ ä¸€ä¸ªæœ‰å¤šæ¡è¯„è®ºçš„æ–‡ä»¶ï¼Œæ ¼å¼æ–‡ä»¶ä¸ºâ€œcsvâ€æ ¼å¼",
                                     type="filepath",
                                     file_count="single",
                                     size="lg")

        # gr.Slider(2, 20, value=4, label="Count", info="Choose between 2 and 20"),

        with gr.Column():
            txt_output = gr.Textbox(label="æƒ…æ„Ÿåˆ†æç»“æœ")
            img_wordcloud = gr.Image(label="è¯äº‘å›¾",
                                     type="filepath",
                                     sources="clipboard")
    with gr.Column(variant="panel"):
        gr.Markdown("# æƒ…ç»ªåˆ†æ")
        with gr.Row():
            with gr.Column(variant="panel"):
                audio_sentiment = gr.Audio(label="æƒ…ç»ªåˆ†æè¯­éŸ³è¾“å…¥",
                                           type="filepath",
                                           value=None)
                img_sentiment = gr.Image(label="æƒ…ç»ªåˆ†æå›¾ç‰‡è¾“å…¥",
                                         type="filepath")
                with gr.Row():
                    audio_btn = gr.Button("è¯­éŸ³æäº¤")
                    img_btn = gr.Button("å›¾ç‰‡æäº¤")
            lab = gr.Label(label="æƒ…ç»ªåˆ†ææ•°å€¼")
    # gr.Slider(2, 20, value=4, label="Count", info="Choose between 2 and 20"),
    up_btn.upload(fn=emotional_analysis.emotional_analysis_csv,
                  inputs=[up_btn, shape],
                  outputs=[txt_output, img_wordcloud])
    txt.submit(fn=emotional_analysis.emotional_analysis_text,
               inputs=txt_emotion,
               outputs=txt_output)
    audio_btn.click(fn=process_audio,
                    inputs=audio_sentiment,
                    outputs=lab)
    img_btn.click(fn=process_img,
                  inputs=img_sentiment,
                  outputs=lab)

with gr.Blocks() as demo_QA:
    gr.Markdown("# æ¬¢è¿è¿›å…¥ QA-GPT! ğŸŒŸğŸš€")

    chatbot = gr.Chatbot(elem_id="chatbot")
    state = gr.State([])
    with gr.Row():
        txt = gr.Textbox(label="æ–‡å­—è¾“å…¥",
                         placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ŒæŒ‰å›è½¦å‘é€",
                         elem_id="textbox")
        # å¾—åˆ°éŸ³é¢‘æ–‡ä»¶åœ°å€
        audio = gr.Audio(sources="microphone",
                         type="filepath",
                         label="è¯­éŸ³è¾“å…¥")
    audio_output = gr.Audio(autoplay=True,
                            visible=False)
    txt.submit(llm_run.agent_run,
               [txt, state],
               [chatbot, state, audio_output])

if __name__ == "__main__":
    # è¿è¡Œbatæ–‡ä»¶ï¼Œè¾“å…¥ %PYTHON% inference_webui.py --model_dir ./logs\OUTPUT_MODEL\G_5900.pth
    load_dotenv(".env")
    openai.api_key = os.environ.get("OPENAI_API_KEY")
    demo = gr.TabbedInterface(
        [demo_Emotion, demo_EmotionalAnalysis, demo_QA],
        tab_names=["æƒ…æ„ŸåŒ–èŠå¤©æœºå™¨äºº", "æƒ…æ„Ÿåˆ†æ", "çŸ¥è¯†é—®ç­”"],
        title="åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æƒ…æ„ŸåŒ–æ™ºèƒ½èŠå¤©æœºå™¨äººç³»ç»Ÿ--åˆ˜æ¿›",
        theme="NoCrypt/miku",

    )
    demo.launch(share=True, inbrowser=True)  # , auth=login.open_login_window
