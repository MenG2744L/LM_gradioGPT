import os
import gradio as gr
import openai
from dotenv import load_dotenv
from src import whisper
from src import llm_run

with gr.Blocks(css="#chatbot{height:800px} .overflow-y-auto{height:800px}") as demo_1:
    gr.Markdown("# Welcome to GradioGPT! ğŸŒŸğŸš€")
    gr.Markdown(
        "An easy to use template. It comes with state and settings managment"
    )
    chatbot = gr.Chatbot(elem_id="chatbot")
    state = gr.State([])
    with gr.Row():
        # å½•éŸ³åŠŸèƒ½
        with gr.Row():
            # å¾—åˆ°éŸ³é¢‘æ–‡ä»¶åœ°å€
            audio = gr.Audio(sources="microphone", type="filepath", label="Input Audio")
        txt = gr.Textbox(show_label=False, placeholder="Enter text and press enter", elem_id="textbox")
    audio_output = gr.Audio(label="Output Audio", autoplay=True)
    txt.submit(llm_run.conversation_run, [txt, state], [chatbot, state, audio_output])
    audio.change(whisper.process_audio, [audio, state], [chatbot, state, audio_output])

with gr.Blocks(css="#chatbot{height:800px} .overflow-y-auto{height:800px}") as demo_2:
    gr.Markdown("# Welcome to QA-GPT! ğŸŒŸğŸš€")
    gr.Markdown(
        "An easy to use template. It comes with state and settings managment"
    )
    chatbot = gr.Chatbot(elem_id="chatbot")
    state = gr.State([])
    with gr.Row():
        # å½•éŸ³åŠŸèƒ½
        with gr.Row():
            # å¾—åˆ°éŸ³é¢‘æ–‡ä»¶åœ°å€
            audio = gr.Audio(sources="microphone", type="filepath", label="Input Audio")
        txt = gr.Textbox(show_label=False, placeholder="Enter text and press enter", elem_id="textbox")
    audio_output = gr.Audio(label="Output Audio", autoplay=True)
    txt.submit(llm_run.conversation_run, [txt, state], [chatbot, state, audio_output])

if __name__ == "__main__":
    load_dotenv(".env")
    openai.api_key = os.environ.get("OPENAI_API_KEY")
    demo = gr.TabbedInterface(
        [demo_1, demo_2],
        tab_names=["æƒ…æ„ŸåŒ–èŠå¤©æœºå™¨äºº", "çŸ¥è¯†é—®ç­”"],
        title="demo"
    )
    demo.launch()
