import os
from typing import Tuple, List

import gradio as gr
import openai
from dotenv import load_dotenv
from src import whisper, login, llm_run, img_to_text


def on_clear_button_click() -> Tuple[str, List]:
    return "", []


with gr.Blocks(css="#chatbot{height:1500px} .overflow-y-auto{height:1500px}") as demo_Emotion:
    gr.Markdown("# Welcome to Emotion-GPT! ğŸŒŸğŸš€")
    gr.Markdown(
        "An easy to use template. It comes with state and settings managment"
    )
    role = gr.Radio(label="è§’è‰²è®¾å®š", choices=["æ— ", "é»˜è®¤", "è¥å…»å¸ˆ", "æ¸¸æˆäººç‰©"], value="é»˜è®¤")
    chatbot = gr.Chatbot(elem_id="chatbot", height="800px", label="èŠå¤©åŒº")
    clear = gr.Button("Clear")
    state = gr.State([])
    with gr.Row():
        txt = gr.Textbox(label="æ–‡å­—è¾“å…¥", placeholder="Enter text and press enter", elem_id="textbox")
        # å¾—åˆ°éŸ³é¢‘æ–‡ä»¶åœ°å€
        audio = gr.Audio(sources="microphone", type="filepath", label="è¯­éŸ³è¾“å…¥")
    img = gr.Image(sources="upload", type="filepath", label="å›¾ç‰‡è¾“å…¥")
    audio_output = gr.Audio(autoplay=True, visible=False)

    img.upload(img_to_text.img2text,
               [img, state],
               [chatbot, state, audio_output])
    txt.submit(llm_run.conversation_run,
               [txt, role, state],
               [chatbot, state, audio_output])
    audio.stop_recording(whisper.process_audio,
                         [audio, role, state],
                         [chatbot, state, audio_output])
    clear.click(on_clear_button_click,
                None,
                [chatbot, state],
                queue=False, )

with gr.Blocks(css="#chatbot{height:800px} .overflow-y-auto{height:800px}") as demo_QA:
    gr.Markdown("# Welcome to QA-GPT! ğŸŒŸğŸš€")
    gr.Markdown(
        "An easy to use template. It comes with state and settings managment"
    )
    chatbot = gr.Chatbot(elem_id="chatbot")
    state = gr.State([])
    with gr.Row():
        txt = gr.Textbox(show_label=False, placeholder="Enter text and press enter", elem_id="textbox")
        # å¾—åˆ°éŸ³é¢‘æ–‡ä»¶åœ°å€
        audio = gr.Audio(sources="microphone", type="filepath", label="Input Audio")
    audio_output = gr.Audio(label="Output Audio", autoplay=True)
    txt.submit(llm_run.conversation_run, [txt, state], [chatbot, state, audio_output])

if __name__ == "__main__":
    # è¿è¡Œbatæ–‡ä»¶ï¼Œè¾“å…¥ %PYTHON% inference_webui.py --model_dir ./logs\OUTPUT_MODEL\G_5900.pth
    load_dotenv(".env")
    openai.api_key = os.environ.get("OPENAI_API_KEY")
    demo = gr.TabbedInterface(
        [demo_Emotion, demo_QA],
        tab_names=["æƒ…æ„ŸåŒ–èŠå¤©æœºå™¨äºº", "çŸ¥è¯†é—®ç­”"],
        title="demo"
    )
    demo.launch(share=False, inbrowser=True)  # , auth=login.open_login_window
